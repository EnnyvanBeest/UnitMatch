{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This Notebook/demo uses spike interface, [here is how to install Spike Interface](https://spikeinterface.readthedocs.io/en/stable/get_started/installation.html)\n",
                "Note that for this demo we used spike interface  version  0.103.0\n",
                "It is possible that spike interface changes and we don't notice. In that case please update the SI related blocks with SI's new ways of extracting raw data and waveforms from raw data according to their ReadMe. The UnitMatch side of things should remain stable. An example blogpost can also be found here: https://olebialas.github.io/posts/2025-10-02-unitmatch/"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the SI environment:\n",
                "\n",
                "`pip install UnitMatchPy`\n",
                "\n",
                "OR in the UM environment:\n",
                "\n",
                "`pip install spikeinterface`\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "import spikeinterface as si\n",
                "import spikeinterface.extractors as se\n",
                "import spikeinterface.preprocessing as spre\n",
                "import UnitMatchPy.extract_raw_data as erd\n",
                "import numpy as np "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Data & get good units"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Spike Interface can load in many different types of Ephys data look [here](https://spikeinterface.readthedocs.io/en/latest/modules/extractors.html) for documentation on function to read in different data formats. [Example data can be found here.](https://figshare.com/articles/dataset/UnitMatch_Demo_-_data/24305758/1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Make list of recordings/sortings to iterate over - depends on whether you have compressed data or not - see SI documentation\n",
                "# recordings = [se.read_spikeglx(r'Path/To/bin/1', stream_name=\"imec0.ap\"), se.read_spikeglx(r'Path/To/bin/2', stream_name=\"imec0.ap\")] # Note, only applicable when you don't have the raw waveforms yet. SKIP when using our demo data.\n",
                "recordings = [se.read_cbin_ibl(r'Path/To/Cbin/1', stream_name=\"ap\"), se.read_cbin_ibl(r'Path/To/Cbin/2/', stream_name=\"ap\")] # you can add more sessions if you want\n",
                "\n",
                "# Note, read_kilosort will only read good units by default. To read all units, use the argument 'load_good_only=False'\n",
                "KS_dirs = [r'/Path/To/KS/1',r'/Path/To/KS/2'] # you can add more sessions if you want\n",
                "sortings = [se.read_kilosort(KS_dirs[0]), se.read_kilosort(KS_dirs[1])] # you can add more sessions if you want\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Process average waveforms / templates\n",
                "\n",
                "Beaware the spike interface method is different to the native unitmatch method in ExtractRawDemo.ipynb or in the MatLab version"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pre-process the raw data\n",
                "# Note, only applicable when you don't have the raw waveforms yet. SKIP when using our demo data. \n",
                "for recording in recordings:\n",
                "    recording = spre.phase_shift(recording) #correct for time delay between recording channels\n",
                "    recording = spre.highpass_filter(recording) #highpass\n",
                "\n",
                "    # for motion correction, this can be very slow\n",
                "    #Uncommented code below to do in session motion correction\n",
                "    #recording = spre.correct_motion(recording, preset=\"nonrigid_fast_and_accurate\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Split each recording/sorting into 2 halves               \n",
                "# # Note, only applicable when you don't have the raw waveforms yet. SKIP when using our demo data.     \n",
                "for i, sorting in enumerate(sortings):\n",
                "    split_idx = recordings[i].get_num_samples() // 2\n",
                "\n",
                "    split_sorting = []\n",
                "    split_sorting.append(sorting.frame_slice(start_frame=0, end_frame=split_idx))\n",
                "    split_sorting.append(sorting.frame_slice(start_frame=split_idx, end_frame=recordings[i].get_num_samples()))\n",
                "\n",
                "    sortings[i] = split_sorting \n",
                "\n",
                "for i, recording in enumerate(recordings):\n",
                "    split_idx = recording.get_num_samples() // 2\n",
                "\n",
                "    split_recording = []\n",
                "    split_recording.append(recording.frame_slice(start_frame=0, end_frame=split_idx))\n",
                "    split_recording.append(recording.frame_slice(start_frame=split_idx, end_frame=recording.get_num_samples()))\n",
                "\n",
                "    recordings[i] = split_recording\n",
                " "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create sorting analyzer for each pair\n",
                "# Note, only applicable when you don't have the raw waveforms yet. SKIP when using our demo data. \n",
                "\n",
                "analysers = []\n",
                "for i in range(len(recordings)):\n",
                "    split_analysers = []\n",
                "\n",
                "    split_analysers.append(si.create_sorting_analyzer(sortings[i][0], recordings[i][0], sparse=False))\n",
                "    split_analysers.append(si.create_sorting_analyzer(sortings[i][1], recordings[i][1], sparse=False))\n",
                "    analysers.append(split_analysers)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#create templates using SortingAnalyzer (SI >= 0.101)\n",
                "# Note, only applicable when you don't have the raw waveforms yet. SKIP when using our demo data. \n",
                "# NOTE: We now use a per-session compute+save pipeline in the next cell\n",
                "# to reduce memory pressure. This cell intentionally does not compute\n",
                "# or store all sessions' waveforms/templates in memory.\n",
                "USE_PER_SESSION_PIPELINE = True\n",
                "print('Using per-session compute+save pipeline. Skip heavy batch compute.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save extracted data in a unit match friendly folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Note, only applicable when you don't have the raw waveforms yet. SKIP when using our demo data. \n",
                "\n",
                "# Per-session compute + save to UnitMatch-friendly folder to keep memory footprint low\n",
                "# Note: If UMInputData already exists, we reuse it.\n",
                "import os, gc, shutil\n",
                "from joblib import parallel_backend\n",
                "from threadpoolctl import threadpool_limits\n",
                "\n",
                "UM_input_dir = os.path.join(os.getcwd(), 'UMInputData')\n",
                "os.makedirs(UM_input_dir, exist_ok=True)\n",
                "all_session_paths = []\n",
                "\n",
                "for i in range(len(analysers)):\n",
                "    session_x_path = os.path.join(UM_input_dir, f'Session{i+1}')\n",
                "    os.makedirs(session_x_path, exist_ok=True)\n",
                "\n",
                "    # 1) Save good unit labels for this session\n",
                "    good_units_path = os.path.join(session_x_path, 'cluster_group.tsv')\n",
                "    channel_positions_path = os.path.join(session_x_path, 'channel_positions.npy')\n",
                "\n",
                "    # Prefer existing 'good_units' if present; otherwise copy from KS output if available\n",
                "    try:\n",
                "        _ = good_units  # check variable exists\n",
                "        save_good_units = np.vstack((np.array(('cluster_id', 'group')), good_units[i]))\n",
                "        save_good_units[0,0] = 0\n",
                "        np.savetxt(good_units_path, save_good_units, fmt=['%d','%s'], delimiter='\t')\n",
                "    except NameError:\n",
                "        ks_label = os.path.join(KS_dirs[i], 'cluster_group.tsv')\n",
                "        if os.path.exists(ks_label):\n",
                "            shutil.copy2(ks_label, good_units_path)\n",
                "        else:\n",
                "            print(f'Warning: no good_units or cluster_group.tsv found for session {i+1}')\n",
                "\n",
                "    # 2) Compute templates for both halves and save waveforms immediately\n",
                "    t_halves = []\n",
                "    for half in range(2):\n",
                "        ana = analysers[i][half]\n",
                "        # preselect spikes at low count to limit memory\n",
                "        ana.compute('random_spikes', method='uniform', max_spikes_per_unit=100)\n",
                "        with threadpool_limits(1), parallel_backend('threading'):\n",
                "            ana.compute('waveforms', ms_before=1.0, ms_after=2.0, dtype='float32',\n",
                "                       n_jobs=2, chunk_duration='100ms', total_memory='128M',\n",
                "                       save=False, progress_bar=True)\n",
                "            ana.compute('templates', n_jobs=2, chunk_duration='100ms', total_memory='128M',\n",
                "                       progress_bar=True)\n",
                "        t = ana.get_extension('templates').get_data()\n",
                "        t_halves.append(t)\n",
                "        # attempt to release extensions to shrink memory\n",
                "        for ext in ('templates','waveforms','random_spikes'):\n",
                "            try:\n",
                "                ana.delete_extension(ext)\n",
                "            except Exception:\n",
                "                try:\n",
                "                    ana.remove_extension(ext)\n",
                "                except Exception:\n",
                "                    pass\n",
                "\n",
                "    avg_waves = np.stack((t_halves[0], t_halves[1]), axis=-1)\n",
                "    # Channel positions (same for both halves)\n",
                "    np.save(channel_positions_path, analysers[i][0].get_channel_locations())\n",
                "\n",
                "    # 3) Save per-session average waveforms in UnitMatch format\n",
                "    all_unit_ids = np.array(analysers[i][0].sorting.get_unit_ids(), dtype=int)\n",
                "    extract_good_only = bool(globals().get('extract_good_units_only', False))\n",
                "\n",
                "    # Derive good unit ids if needed\n",
                "    good_ids = all_unit_ids\n",
                "    if extract_good_only:\n",
                "        try:\n",
                "            if 'good_units' in globals():\n",
                "                gu = good_units[i]\n",
                "                if getattr(gu, 'ndim', 1) == 2 and gu.shape[1] >= 2:\n",
                "                    good_ids = gu[gu[:, 1] == 'good', 0].astype(int)\n",
                "                else:\n",
                "                    good_ids = np.array(gu, dtype=int).ravel()\n",
                "            else:\n",
                "                # Fallback: read the saved TSV\n",
                "                tbl = np.genfromtxt(good_units_path, delimiter='\\t', names=True, dtype=None, encoding='utf-8')\n",
                "                good_ids = tbl['cluster_id'][tbl['group'] == 'good'].astype(int)\n",
                "        except Exception as e:\n",
                "            print(f'Warning: could not parse good units for session {i+1}: {e}; saving all units.')\n",
                "            extract_good_only = False\n",
                "            good_ids = all_unit_ids\n",
                "\n",
                "    erd.save_avg_waveforms(avg_waves, session_x_path, all_unit_ids, good_ids, extract_good_units_only=extract_good_only)\n",
                "\n",
                "\n",
                "    # 4) Free memory aggressively before next session\n",
                "    t_halves = None; avg_waves = None\n",
                "    try:\n",
                "        analysers[i] = [None, None]\n",
                "    except Exception:\n",
                "        pass\n",
                "    try:\n",
                "        recordings[i] = None; sortings[i] = None\n",
                "    except Exception:\n",
                "        pass\n",
                "    gc.collect()\n",
                "\n",
                "    all_session_paths.append(session_x_path)\n",
                "\n",
                "print('Per-session saving complete. Proceed to Run UnitMatch section.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run UnitMatch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload \n",
                "\n",
                "import UnitMatchPy.bayes_functions as bf\n",
                "import UnitMatchPy.utils as util\n",
                "import UnitMatchPy.overlord as ov\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import UnitMatchPy.save_utils as su\n",
                "import UnitMatchPy.GUI as gui\n",
                "import UnitMatchPy.assign_unique_id as aid\n",
                "import UnitMatchPy.default_params as default_params"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#get default parameters, can add your own before or after!\n",
                "\n",
                "# default of Spikeinterface as by default spike interface extracts waveforms in a different manner.\n",
                "param = {'SpikeWidth': 90, 'waveidx': np.arange(20,50), 'PeakLoc': 35}\n",
                "param = default_params.get_default_param()\n",
                "\n",
                "# Point UnitMatch to the per-session UMInputData folders we just created\n",
                "KS_dirs = all_session_paths\n",
                "param['KS_dirs'] = KS_dirs\n",
                "wave_paths, unit_label_paths, channel_pos = util.paths_from_KS(KS_dirs)\n",
                "param = util.get_probe_geometry(channel_pos[0], param)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def zero_center_waveform(waveform):\n",
                "    \"\"\"\n",
                "    Centers waveform about zero, by subtracting the mean of the first 15 time points.\n",
                "    This function is useful for Spike Interface where the waveforms are not centered about 0.\n",
                "\n",
                "    Arguments:\n",
                "        waveform - ndarray (nUnits, Time Points, Channels, CV)\n",
                "\n",
                "    Returns:\n",
                "        Zero centered waveform\n",
                "    \"\"\"\n",
                "    waveform = waveform -  np.broadcast_to(waveform[:,:15,:,:].mean(axis=1)[:, np.newaxis,:,:], waveform.shape)\n",
                "    return waveform"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#read in data and select the good units and exact metadata\n",
                "waveform, session_id, session_switch, within_session, good_units, param = util.load_good_waveforms(wave_paths, unit_label_paths, param, good_units_only = True) \n",
                "\n",
                "#param['peak_loc'] = #may need to set as a value if the peak location is NOT ~ half the spike width\n",
                "\n",
                "# create clus_info, contains all unit id/session related info\n",
                "clus_info = {'good_units' : good_units, 'session_switch' : session_switch, 'session_id' : session_id, \n",
                "            'original_ids' : np.concatenate(good_units) }\n",
                "\n",
                "#Extract parameters from waveform\n",
                "extracted_wave_properties = ov.extract_parameters(waveform, channel_pos, clus_info, param)\n",
                "\n",
                "#Extract metric scores\n",
                "total_score, candidate_pairs, scores_to_include, predictors  = ov.extract_metric_scores(extracted_wave_properties, session_switch, within_session, param, niter  = 2)\n",
                "\n",
                "#Probability analysis\n",
                "prior_match = 1 - (param['n_expected_matches'] / param['n_units']**2 ) # freedom of choose in prior prob\n",
                "priors = np.array((prior_match, 1-prior_match))\n",
                "\n",
                "labels = candidate_pairs.astype(int)\n",
                "cond = np.unique(labels)\n",
                "score_vector = param['score_vector']\n",
                "parameter_kernels = np.full((len(score_vector), len(scores_to_include), len(cond)), np.nan)\n",
                "\n",
                "parameter_kernels = bf.get_parameter_kernels(scores_to_include, labels, cond, param, add_one = 1)\n",
                "\n",
                "probability = bf.apply_naive_bayes(parameter_kernels, priors, predictors, param, cond)\n",
                "\n",
                "output_prob_matrix = probability[:,1].reshape(param['n_units'],param['n_units'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "util.evaluate_output(output_prob_matrix, param, within_session, session_switch, match_threshold = 0.75)\n",
                "\n",
                "match_threshold = param['match_threshold']\n",
                "#match_threshold = try different values here!\n",
                "\n",
                "output_threshold = np.zeros_like(output_prob_matrix)\n",
                "output_threshold[output_prob_matrix > match_threshold] = 1\n",
                "\n",
                "plt.imshow(output_threshold, cmap = 'Greys')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for k in ['amplitude','spatial_decay','avg_centroid','avg_waveform',\n",
                "          'avg_waveform_per_tp','good_wave_idxs','max_site','max_site_mean']:\n",
                "    v = extracted_wave_properties[k]\n",
                "    try:\n",
                "        print(k, getattr(v, 'shape', None), type(v))\n",
                "    except Exception as e:\n",
                "        print(k, type(v), e)\n",
                "\n",
                "\n",
                "amplitude = extracted_wave_properties['amplitude']\n",
                "spatial_decay = extracted_wave_properties['spatial_decay']\n",
                "avg_centroid = extracted_wave_properties['avg_centroid']\n",
                "avg_waveform = extracted_wave_properties['avg_waveform']\n",
                "avg_waveform_per_tp = extracted_wave_properties['avg_waveform_per_tp']\n",
                "wave_idx = extracted_wave_properties['good_wave_idxs']\n",
                "max_site = extracted_wave_properties['max_site']\n",
                "max_site_mean = extracted_wave_properties['max_site_mean']\n",
                "gui.process_info_for_GUI(output_prob_matrix, match_threshold, scores_to_include, total_score, amplitude, spatial_decay,\n",
                "                         avg_centroid, avg_waveform, avg_waveform_per_tp, wave_idx, max_site, max_site_mean, \n",
                "                         waveform, within_session, channel_pos, clus_info, param)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "is_match, not_match, matches_GUI = gui.run_GUI()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#this function has 2 mode 'And' 'Or', which returns a matches if they appear in both or one cv pair\n",
                "#then it will add all the matches selected as IsMaatch, then remove all matches in NotMatch\n",
                "matches_curated = util.curate_matches(matches_GUI, is_match, not_match, mode = 'And')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "matches = np.argwhere(match_threshold == 1)\n",
                "UIDs = aid.assign_unique_id(output_prob_matrix, param, clus_info)\n",
                "\n",
                "save_dir = r'Path/To/save/directory'\n",
                "#NOTE - change to matches to matches_curated if done manual curation with the GUI\n",
                "su.save_to_output(save_dir, scores_to_include, matches # matches_curated\n",
                "                  , output_prob_matrix, avg_centroid, avg_waveform, avg_waveform_per_tp, max_site,\n",
                "                   total_score, output_threshold, clus_info, param, UIDs = UIDs, matches_curated = None, save_match_table = True)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "DUM",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
