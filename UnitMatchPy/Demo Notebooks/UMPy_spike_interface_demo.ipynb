{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook/demo uses spike interface, [here is how to install Spike Interface](https://spikeinterface.readthedocs.io/en/stable/get_started/installation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the SI environment:\n",
    "\n",
    "`pip install UnitMatchPy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import spikeinterface as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "import unitmatchpy.extract_raw_data as erd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & get good units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spike Interface can load in many different types of Ephys data look [here](https://spikeinterface.readthedocs.io/en/latest/modules/extractors.html) for documentation on function to read in different data formats. [Example data can be found here.](https://figshare.com/articles/dataset/UnitMatch_Demo_-_data/24305758/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make list of recordings/sortings to iterate over\n",
    "recordings = [se.read_spikeglx(r'path/to/SpikeGLX/data', stream_name=\"imec0.ap\")]\n",
    "\n",
    "sortings = [se.read_kilosort(r'Path/To/KiloSort/Directory')]\n",
    "\n",
    "#Will only make average waveforms for good units\n",
    "extract_good_units_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting good units only\n",
    "sortings[0].get_property_keys() #lists keys for attached properties if 'quality' is not suitable\n",
    "\n",
    "#Good units which will be used in Unit Match\n",
    "good_units = []\n",
    "units_used = []\n",
    "for i, sorting in enumerate(sortings):\n",
    "    unit_ids_tmp = sorting.get_property('original_cluster_id')\n",
    "    is_good_tmp = sorting.get_property('quality')\n",
    "    good_units.append(np.stack((unit_ids_tmp,is_good_tmp), axis = 1))\n",
    "\n",
    "    units_used.append(unit_ids_tmp)\n",
    "    if extract_good_units_only is True:\n",
    "        keep = np.argwhere(is_good_tmp == 'good').squeeze()\n",
    "        sortings[i] = sorting.select_units(keep)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process average waveforms / templates\n",
    "\n",
    "Beaware the spike interface method is different to the native unitmatch method in ExtractRawDemo.ipynb or in the MatLab version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the raw data\n",
    "for recording in recordings:\n",
    "    recording = spre.phase_shift(recording) #correct for time delay between recording channels\n",
    "    recording = spre.highpass_filter(recording) #highpass\n",
    "\n",
    "    # for motion correction, this can be very slow\n",
    "    #Uncommented code below to do in session motion correction\n",
    "    #recording = spre.correct_motion(recording, preset=\"nonrigid_fast_and_accurate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split each recording/sorting into 2 halves                    \n",
    "for i, sorting in enumerate(sortings):\n",
    "    split_idx = recordings[i].get_num_samples() // 2\n",
    "\n",
    "    split_sorting = []\n",
    "    split_sorting.append(sorting.frame_slice(start_frame=0, end_frame=split_idx))\n",
    "    split_sorting.append(sorting.frame_slice(start_frame=split_idx, end_frame=recordings[i].get_num_samples()))\n",
    "\n",
    "    sortings[i] = split_sorting \n",
    "\n",
    "for i, recording in enumerate(recordings):\n",
    "    split_idx = recording.get_num_samples() // 2\n",
    "\n",
    "    split_recording = []\n",
    "    split_recording.append(recording.frame_slice(start_frame=0, end_frame=split_idx))\n",
    "    split_recording.append(recording.frame_slice(start_frame=split_idx, end_frame=recording.get_num_samples()))\n",
    "\n",
    "    recordings[i] = split_recording\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sorting analyzer for each pair\n",
    "analysers = []\n",
    "for i in range(len(recordings)):\n",
    "    split_analysers = []\n",
    "\n",
    "    split_analysers.append(si.create_sorting_analyzer(sortings[i][0], recordings[i][0], sparse=False))\n",
    "    split_analysers.append(si.create_sorting_analyzer(sortings[i][1], recordings[i][1], sparse=False))\n",
    "    analysers.append(split_analysers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the fast template extension for each sorting analyser\n",
    "all_waveforms = []\n",
    "for i in range(len(analysers)):\n",
    "    for half in range(2):\n",
    "        analysers[i][half].compute(\n",
    "            \"random_spikes\",\n",
    "            method=\"uniform\",\n",
    "            max_spikes_per_unit=500)\n",
    "        \n",
    "        #Analysers[i][half].compute('fast_templates', n_jobs = 0.8,  return_scaled=True)\n",
    "        analysers[i][half].compute('fast_templates', n_jobs = 0.8)\n",
    "    \n",
    "    templates_first = analysers[i][0].get_extension('fast_templates')\n",
    "    templates_second = analysers[i][1].get_extension('fast_templates')\n",
    "    t1 = templates_first.get_data()\n",
    "    t2 = templates_second.get_data()\n",
    "    all_waveforms.append(np.stack((t1,t2), axis = -1))\n",
    "\n",
    "#Make a channel_positions array\n",
    "all_positions = []\n",
    "for i in range(len(analysers)):\n",
    "    #positions for first half and second half are the same\n",
    "    all_positions.append(analysers[i][0].get_channel_locations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save extracted data in a unit match friendly folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "UM_input_dir = os.path.join(os.getcwd(), 'UMInputData')\n",
    "\n",
    "os.mkdir(UM_input_dir)\n",
    "\n",
    "all_session_paths = []\n",
    "for i in range(len(recordings)):\n",
    "    session_x_path = os.path.join(UM_input_dir, f'Session{i+1}') #lets start at 1\n",
    "    os.mkdir(session_x_path)\n",
    "\n",
    "    #save the GoodUnits as a .rsv first column is unit ID,second is 'good' or 'mua'\n",
    "    good_units_path = os.path.join(session_x_path, 'cluster_group.tsv')\n",
    "    channel_positions_path = os.path.join(session_x_path, 'channel_positions.npy')\n",
    "    save_good_units = np.vstack((np.array(('cluster_id', 'group')), good_units[i])) #Title of colum one is '0000' Not 'cluster_id')\n",
    "    save_good_units[0,0] = 0 # need to be int to use np.savetxt \n",
    "    np.savetxt(good_units_path, save_good_units, fmt =['%d','%s'], delimiter='\\t')\n",
    "    if extract_good_units_only:\n",
    "        Units = np.argwhere(good_units[0][:,1] == 'good')\n",
    "        erd.save_avg_waveforms(all_waveforms[i], session_x_path, Units, ExtractGoodUnitsOnly = extract_good_units_only)\n",
    "    else:\n",
    "        erd.save_avg_waveforms(all_waveforms[i], session_x_path, good_units[i], ExtractGoodUnitsOnly = extract_good_units_only)\n",
    "    np.save(channel_positions_path, all_positions[i])\n",
    "\n",
    "    all_session_paths.append(session_x_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run UnitMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "\n",
    "import UnitMatchPy.bayes_functions as bf\n",
    "import UnitMatchPy.utils as util\n",
    "import UnitMatchPy.overlord as ov\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import UnitMatchPy.save_utils as su\n",
    "import UnitMatchPy.GUI as gui\n",
    "import UnitMatchPy.assign_unique_id as aid\n",
    "import UnitMatchPy.default_params as default_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get default parameters, can add your own before or after!\n",
    "\n",
    "# default of Spikeinterface as by default spike interface extracts waveforms in a different manner.\n",
    "param = {'SpikeWidth': 90, 'waveidx': np.arange(20,50), 'PeakLoc': 35}\n",
    "param = default_params.get_default_param()\n",
    "\n",
    "KS_dirs = [r'path/to/KSdir/Session1', r'Path/to/KSdir/Session2']\n",
    "\n",
    "param['KS_dirs'] = KS_dirs\n",
    "wave_paths, unit_label_paths, channel_pos = util.paths_from_KS(KS_dirs)\n",
    "param = util.get_probe_geometry(channel_pos[0], param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_center_waveform(waveform):\n",
    "    \"\"\"\n",
    "    Centers waveform about zero, by subtracting the mean of the first 15 time points.\n",
    "    This function is useful for Spike Interface where the waveforms are not centered about 0.\n",
    "\n",
    "    Arguments:\n",
    "        waveform - ndarray (nUnits, Time Points, Channels, CV)\n",
    "\n",
    "    Returns:\n",
    "        Zero centered waveform\n",
    "    \"\"\"\n",
    "    waveform = waveform -  np.broadcast_to(waveform[:,:15,:,:].mean(axis=1)[:, np.newaxis,:,:], waveform.shape)\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data and select the good units and exact metadata\n",
    "waveform, session_id, session_switch, within_session, good_units, param = util.load_good_waveforms(wave_paths, unit_label_paths, param, good_units_only = True) \n",
    "\n",
    "#param['peak_loc'] = #may need to set as a value if the peak location is NOT ~ half the spike width\n",
    "\n",
    "# create clus_info, contains all unit id/session related info\n",
    "clus_info = {'good_units' : good_units, 'session_switch' : session_switch, 'sessions_id' : session_id, \n",
    "            'original_ids' : np.concatenate(good_units) }\n",
    "\n",
    "#Extract parameters from waveform\n",
    "extracted_wave_properties = ov.extract_parameters(waveform, channel_pos, clus_info, param)\n",
    "\n",
    "#Extract metric scores\n",
    "total_score, candidate_pairs, scores_to_include, predictors  = ov.extract_metric_scores(extracted_wave_properties, session_switch, within_session, param, niter  = 2)\n",
    "\n",
    "#Probability analysis\n",
    "prior_match = 1 - (param['n_expected_matches'] / param['n_units']**2 ) # freedom of choose in prior prob\n",
    "priors = np.array((prior_match, 1-prior_match))\n",
    "\n",
    "labels = candidate_pairs.astype(int)\n",
    "cond = np.unique(labels)\n",
    "score_vector = param['score_vector']\n",
    "parameter_kernels = np.full((len(score_vector), len(scores_to_include), len(cond)), np.nan)\n",
    "\n",
    "parameter_kernels = bf.get_parameter_kernels(scores_to_include, labels, cond, param, add_one = 1)\n",
    "\n",
    "probability = bf.apply_naive_bayes(parameter_kernels, priors, predictors, param, cond)\n",
    "\n",
    "output_prob_matrix = probability[:,1].reshape(param['n_units'],param['n_units'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.evaluate_output(output_prob_matrix, param, within_session, session_switch, match_threshold = 0.75)\n",
    "\n",
    "match_threshold = param['match_threshold']\n",
    "#match_threshold = try different values here!\n",
    "\n",
    "output_threshold = np.zeros_like(output_prob_matrix)\n",
    "output_threshold[output_prob_matrix > match_threshold] = 1\n",
    "\n",
    "plt.imshow(output_threshold, cmap = 'Greys')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude = extracted_wave_properties['amplitude']\n",
    "spatial_decay = extracted_wave_properties['spatial_decay']\n",
    "avg_centroid = extracted_wave_properties['avg_centroid']\n",
    "avg_waveform = extracted_wave_properties['avg_waveform']\n",
    "avg_waveform_per_tp = extracted_wave_properties['avg_waveform_per_tp']\n",
    "wave_idx = extracted_wave_properties['good_wave_idxs']\n",
    "max_site = extracted_wave_properties['max_site']\n",
    "max_site_mean = extracted_wave_properties['max_site_mean']\n",
    "gui.process_info_for_GUI(output_prob_matrix, match_threshold, scores_to_include, total_score, amplitude, spatial_decay,\n",
    "                         avg_centroid, avg_waveform, avg_waveform_per_tp, wave_idx, max_site, max_site_mean, \n",
    "                         waveform, within_session, channel_pos, clus_info, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_match, not_match, matches_GUI = gui.run_GUI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function has 2 mode 'And' 'Or', which returns a matches if they appear in both or one cv pair\n",
    "#then it will add all the matches selected as IsMaatch, then remove all matches in NotMatch\n",
    "matches_curated = util.curate_matches(matches_GUI, is_match, not_match, mode = 'And')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = np.argwhere(match_threshold == 1)\n",
    "UIDs = aid.assign_unique_id(output_prob_matrix, param, clus_info)\n",
    "\n",
    "save_dir = r'Path/to/save/directory'\n",
    "#NOTE - change to matches to matches_curated if done manual curation with the GUI\n",
    "su.save_to_output(save_dir, scores_to_include, matches # matches_curated\n",
    "                  , output_prob_matrix, avg_centroid, avg_waveform, avg_waveform_per_tp, max_site,\n",
    "                   total_score, output_threshold, clus_info, param, UIDs = UIDs, matches_curated = None, save_match_table = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
