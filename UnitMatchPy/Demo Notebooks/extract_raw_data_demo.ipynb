{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This demo notebook can be used to (optionally) decompress ephys data and create two average waveforms per session needed for Unit Match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import UnitMatchPy.extract_raw_data as erd\n",
    "import numpy as np \n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional, decompress compressed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVE a list of dirs, for where the raw compressed data is e.g .cbim, .ch and .meta files\n",
    "raw_data_dir_paths = [r'Path\\\\to\\\\rawdata\\\\Session1', r'Path\\\\to\\\\rawdata\\\\Session2']\n",
    "#Path to a directory where you want the decompresed data to be saved\n",
    "#this is a large file, using a fast ssd is advised for quicker run times\n",
    "\n",
    "#GIVE a path to a directory where the Decompressed data will be saved\n",
    "#this will make folder called session n for each session\n",
    "decomp_data_save_dir = r'path/to/decompressed/data/save/dir'\n",
    "cbin_paths, ch_paths, meta_paths = erd.get_raw_data_paths(raw_data_dir_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompress Data\n",
    "from mtscomp import Reader\n",
    "\n",
    "decomp_dir = os.path.join(decomp_data_save_dir, 'DecompData')\n",
    "if not os.path.exists(decomp_dir):\n",
    "    os.mkdir(decomp_dir)  # Create a folder in the directory called 'DecompData'\n",
    "\n",
    "data_paths = []\n",
    "for i in range(len(raw_data_dir_paths)):\n",
    "    session_dir = os.path.join(decomp_dir, f'Session{i+1}')  # +1 so starts at 1\n",
    "    if not os.path.exists(session_dir):\n",
    "        os.mkdir(session_dir)  # Make a folder for each session called 'SessionX'\n",
    "    tmpPath = os.path.join(session_dir, 'RawData.bin')\n",
    "    data_paths.append(tmpPath)\n",
    "\n",
    "    # Check if the decompressed data file already exists\n",
    "    if not os.path.exists(tmpPath):\n",
    "        # Create .bin with the decompressed data\n",
    "        r = Reader(check_after_decompress=False)  # Skip the verification check to save time\n",
    "        r.open(cbin_paths[i], ch_paths[i])\n",
    "        r.tofile(tmpPath)\n",
    "        r.close()\n",
    "    else:\n",
    "        print(f\"Decompressed data for Session{i+1} already exists. Skipping decompression.\")\n",
    "\n",
    "# Continue with the rest of your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give paramaters and paths needed for extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Up Parameters\n",
    "sample_amount = 1000 # for both CV, at least 500 per CV\n",
    "spike_width = 82 # assuming 30khz sampling, 82 and 61 are common choices, covers the AP and space around needed for processing\n",
    "half_width = np.floor(spike_width/2).astype(int)\n",
    "max_width = np.floor(spike_width/2).astype(int) #Size of area at start and end of recording to ignore to get only full spikes\n",
    "n_channels = 384 #neuropixels default\n",
    "extract_good_units_only = False # bool, set to true if you want to only extract units marked as good \n",
    "\n",
    "KS4_data = True #bool, set to true if using Kilosort\n",
    "if KS4_data:\n",
    "    spike_width = 61\n",
    "    samples_before = 20\n",
    "    samples_after = spike_width - samples_before\n",
    "\n",
    "#List of paths to a KS directory, can pass paths \n",
    "KS_dirs = [r'path/to/KSdir/Session1', r'Path/to/KSdir/Session2']\n",
    "n_sessions = len(KS_dirs) #How many session are being extracted\n",
    "spike_ids, spike_times, good_units, all_unit_ids = erd.extract_KS_data(KS_dirs, extract_good_units_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have not decompressed data above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give metadata + Raw data paths\n",
    "#if you are NOT decompressing data here, provide a list of paths to the decompressed data and the metadata\n",
    "\n",
    "#data_paths = [r'path/to/Decompressed/data1.bin', r'path/to/Decompressed/data2.bin']\n",
    "#meta_paths = [r''path/to/data.meta', r'path/to/data.meta']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the units \n",
    "\n",
    "if extract_good_units_only:\n",
    "    for sid in range(n_sessions):\n",
    "        # Load metadata\n",
    "        meta_data = erd.read_meta(Path(meta_paths[sid]))\n",
    "        n_elements = int(meta_data['fileSizeBytes']) / 2\n",
    "        n_channels_tot = int(meta_data['nSavedChans'])\n",
    "\n",
    "        # Create memmap to raw data, for that session\n",
    "        data = np.memmap(data_paths[sid], dtype='int16', shape=(int(n_elements / n_channels_tot), n_channels_tot))\n",
    "\n",
    "        # Remove spikes which won't have a full waveform recorded\n",
    "        spike_ids_tmp = np.delete(spike_ids[sid], np.logical_or((spike_times[sid] < max_width), (spike_times[sid] > (data.shape[0] - max_width))))\n",
    "        spike_times_tmp = np.delete(spike_times[sid], np.logical_or((spike_times[sid] < max_width), (spike_times[sid] > (data.shape[0] - max_width))))\n",
    "\n",
    "        # Might be slow extracting sample for good units only?\n",
    "        sample_idx = erd.get_sample_idx(spike_times_tmp, spike_ids_tmp, sample_amount, units=good_units[sid])\n",
    "\n",
    "        if KS4_data:\n",
    "            avg_waveforms = Parallel(n_jobs=-1, verbose=10, mmap_mode='r', max_nbytes=None)(\n",
    "                delayed(erd.extract_a_unit_KS4)(sample_idx[uid], data, samples_before, samples_after, spike_width, n_channels, sample_amount)\n",
    "                for uid in range(good_units[sid].shape[0])\n",
    "            )\n",
    "            avg_waveforms = np.asarray(avg_waveforms)\n",
    "        else:\n",
    "            avg_waveforms = Parallel(n_jobs=-1, verbose=10, mmap_mode='r', max_nbytes=None)(\n",
    "                delayed(erd.extract_a_unit)(sample_idx[uid], data, half_width, spike_width, n_channels, sample_amount)\n",
    "                for uid in range(good_units[sid].shape[0])\n",
    "            )\n",
    "            avg_waveforms = np.asarray(avg_waveforms)\n",
    "\n",
    "        # Save in file named 'RawWaveforms' in the KS Directory\n",
    "        erd.save_avg_waveforms(avg_waveforms, KS_dirs[sid], all_unit_ids[sid], good_units=good_units[sid], extract_good_units_only=extract_good_units_only)\n",
    "\n",
    "else:\n",
    "    for sid in range(n_sessions):\n",
    "        # Extracting ALL the Units\n",
    "        n_units = len(np.unique(spike_ids[sid]))\n",
    "        # Load metadata\n",
    "        meta_data = erd.read_meta(Path(meta_paths[sid]))\n",
    "        n_elements = int(meta_data['fileSizeBytes']) / 2\n",
    "        n_channels_tot = int(meta_data['nSavedChans'])\n",
    "\n",
    "        # Create memmap to raw data, for that session\n",
    "        data = np.memmap(data_paths[sid], dtype='int16', shape=(int(n_elements / n_channels_tot), n_channels_tot))\n",
    "\n",
    "        # Remove spikes which won't have a full waveform recorded\n",
    "        spike_ids_tmp = np.delete(spike_ids[sid], np.logical_or((spike_times[sid] < max_width), (spike_times[sid] > (data.shape[0] - max_width))))\n",
    "        spike_times_tmp = np.delete(spike_times[sid], np.logical_or((spike_times[sid] < max_width), (spike_times[sid] > (data.shape[0] - max_width))))\n",
    "\n",
    "        # Extract sample indices for all units\n",
    "        sample_idx = erd.get_sample_idx(spike_times_tmp, spike_ids_tmp, sample_amount, units=np.unique(spike_ids[sid]))\n",
    "\n",
    "        if KS4_data:\n",
    "            avg_waveforms = Parallel(n_jobs=-1, verbose=10, mmap_mode='r', max_nbytes=None)(\n",
    "                delayed(erd.extract_a_unit_KS4)(sample_idx[uid], data, samples_before, samples_after, spike_width, n_channels, sample_amount)\n",
    "                for uid in range(n_units)\n",
    "            )\n",
    "            avg_waveforms = np.asarray(avg_waveforms)\n",
    "        else:\n",
    "            avg_waveforms = Parallel(n_jobs=-1, verbose=10, mmap_mode='r', max_nbytes=None)(\n",
    "                delayed(erd.extract_a_unit)(sample_idx[uid], data, half_width, spike_width, n_channels, sample_amount)\n",
    "                for uid in range(n_units)\n",
    "            )\n",
    "            avg_waveforms = np.asarray(avg_waveforms)\n",
    "\n",
    "        # Save in file named 'RawWaveforms' in the KS Directory\n",
    "        erd.save_avg_waveforms(avg_waveforms, KS_dirs[sid], all_unit_ids[sid], good_units=good_units[sid], extract_good_units_only=extract_good_units_only)\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: delete the decompressed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "#DELETE the decompressed data Directory/Folder ( i.e multiple sessions)\n",
    "shutil.rmtree(decomp_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UMPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
