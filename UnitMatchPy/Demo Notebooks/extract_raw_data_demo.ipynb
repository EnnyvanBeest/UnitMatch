{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This demo notebook can be used to (optionally) decompress ephys data and create two average waveforms per session needed for Unit Match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import UnitMatchPy.extract_raw_data as erd\n",
    "import numpy as np \n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional, decompress compressed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVE a list of dirs, for where the raw compressed data is e.g .cbim, .ch and .meta files\n",
    "raw_data_dir_paths = [r'Path\\to\\rawdata\\Session1', r'Path\\to\\rawdata\\Session2']\n",
    "\n",
    "#Path to a directory where you want the decompresed data to be saved\n",
    "#this is a large file, using a fast ssd is advised for quicker run times\n",
    "\n",
    "#GIVE a path to a directory where the Decompressed data will be saved\n",
    "#this will make folder called session n for each session\n",
    "decomp_data_save_dir = r'path/to/decompressed/data/save/dir'\n",
    "\n",
    "cbin_paths, ch_paths, meta_paths = erd.get_raw_data_paths(raw_data_dir_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decompress Data\n",
    "from mtscomp import Reader\n",
    "\n",
    "decomp_dir = os.path.join(decomp_data_save_dir, 'DecompData')\n",
    "os.mkdir(decomp_dir) # Create a folder in the directory called 'DecompData'\n",
    "\n",
    "data_paths = []\n",
    "for i in range(len(raw_data_dir_paths)):\n",
    "    tmpPath = os.path.join(decomp_dir, f'Session{i+1}')  #+1 so starts at 1\n",
    "    os.mkdir(tmpPath) # make a folder for each session called 'SessionX' \n",
    "    tmpPath = os.path.join(tmpPath, 'RawData.bin')\n",
    "    data_paths.append(tmpPath)\n",
    "\n",
    "    # create .bin with the decompressed data\n",
    "\n",
    "    #r = Reader() # do the mtscomp verification\n",
    "    r = Reader(check_after_decompress = False) #Skip the verification check to save time\n",
    "    r.open(cbin_paths[i], ch_paths[i])\n",
    "    r.tofile(tmpPath)\n",
    "    r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give paramaters and paths needed for extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Up Parameters\n",
    "sample_amount = 1000 # for both CV, at least 500 per CV\n",
    "spike_width = 82 # assuming 30khz sampling, 82 and 61 are common choices, covers the AP and space around needed for processing\n",
    "half_width = np.floor(spike_width/2).astype(int)\n",
    "n_channels = 384 #neuropixels default\n",
    "extract_good_units_only = False # bool, set to true if you want to only extract units marked as good \n",
    "\n",
    "KS4_data = False #bool, set to true if using Kilosort, as KS4 spike times refer to start of waveform not peak\n",
    "if KS4_data:\n",
    "    samples_before = 20\n",
    "    samples_after = spike_width - samples_before\n",
    "\n",
    "#List of paths to a KS directory, can pass paths \n",
    "KS_dirs = [r'path/to/KiloSort/Dir/Session1', r'path/to/KiloSort/Dir/Session2']\n",
    "n_sessions = len(KS_dirs) #How many session are being extracted\n",
    "spike_ids, spike_times, good_units = erd.extract_KS_data(KS_dirs, extract_good_units_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have not decompressed data above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give metadata + Raw data paths\n",
    "#if you are NOT decompressing data here, provide a list of paths to the decompressed data and the metadata\n",
    "\n",
    "#DataPaths = [r'path/to/Decompressed/data1.bin', r'path/to/Decompressed/data2.bin']\n",
    "#metaPaths = [r''path/to/data.meta', r'path/to/data.meta']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_units = [None for s in range(n_sessions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the units \n",
    "\n",
    "if extract_good_units_only:\n",
    "    for sid in range(n_sessions):\n",
    "        #load metadata\n",
    "        meta_data = erd.Read_Meta(Path(meta_paths[sid]))\n",
    "        n_elements = int(meta_data['fileSizeBytes']) / 2\n",
    "        n_channels_tot = int(meta_data['nSavedChans'])\n",
    "\n",
    "        #create memmap to raw data, for that session\n",
    "        data = np.memmap(data_paths[sid], dtype = 'int16', shape =(int(n_elements / n_channels_tot), n_channels_tot))\n",
    "\n",
    "        # Remove spike which won't have a full waveform recorded\n",
    "        spike_ids_tmp = np.delete(spike_ids[sid], np.logical_or( (spike_times[sid] < half_width), ( spike_times[sid] > (data.shape[0] - half_width))))\n",
    "        spike_times_tmp = np.delete(spike_times[sid], np.logical_or( (spike_times[sid] < half_width), ( spike_times[sid] > (data.shape[0] - half_width))))\n",
    "\n",
    "\n",
    "        #might be slow extracting sample for good units only?\n",
    "        sample_idx = erd.get_sample_idx(spike_times_tmp, spike_ids_tmp, sample_amount, units = good_units[sid])\n",
    "\n",
    "        if KS4_data:\n",
    "            avg_waveforms = Parallel(n_jobs = -1, verbose = 10, mmap_mode='r', max_nbytes=None )(delayed(erd.extract_a_unit_KS4)(sample_idx[uid], data, samples_before, samples_after, spike_width, n_channels, sample_amount)for uid in range(good_units[sid].shape[0]))\n",
    "            avg_waveforms = np.asarray(avg_waveforms)           \n",
    "        else:\n",
    "            avg_waveforms = Parallel(n_jobs = -1, verbose = 10, mmap_mode='r', max_nbytes=None )(delayed(erd.extract_a_unit)(sample_idx[uid], data, half_width, spike_width, n_channels, sample_amount)for uid in range(good_units[sid].shape[0]))\n",
    "            avg_waveforms = np.asarray(avg_waveforms)\n",
    "\n",
    "        #Save in file named 'RawWaveforms' in the KS Directory\n",
    "        erd.save_avg_waveforms(avg_waveforms, KS_dirs[sid], GoodUnits = good_units[sid], extract_good_units_only = extract_good_units_only)\n",
    "\n",
    "else:\n",
    "    for sid in range(n_sessions):\n",
    "        #Extracting ALL the Units\n",
    "        nUnits = len(np.unique(spike_ids[sid]))\n",
    "        #load metadata\n",
    "        meta_data = erd.Read_Meta(Path(meta_paths[sid]))\n",
    "        n_elements = int(meta_data['fileSizeBytes']) / 2\n",
    "        n_channels_tot = int(meta_data['nSavedChans'])\n",
    "\n",
    "        #create memmap to raw data, for that session\n",
    "        data = np.memmap(data_paths[sid], dtype = 'int16', shape =(int(n_elements / n_channels_tot), n_channels_tot))\n",
    "\n",
    "        # Remove spike which won't have a full wavefunction recorded\n",
    "        spike_ids_tmp = np.delete(spike_ids[sid], np.logical_or( (spike_times[sid] < half_width), ( spike_times[sid] > (data.shape[0] - half_width))))\n",
    "        spike_times_tmp = np.delete(spike_times[sid], np.logical_or( (spike_times[sid] < half_width), ( spike_times[sid] > (data.shape[0] - half_width))))\n",
    "\n",
    "\n",
    "        sample_idx = erd.get_sample_idx(spike_times_tmp, spike_ids_tmp, sample_amount, units= np.unique(spike_ids[sid]))\n",
    "        \n",
    "        if KS4_data:\n",
    "            avg_waveforms = Parallel(n_jobs = -1, verbose = 10, mmap_mode='r', max_nbytes=None )(delayed(erd.extract_a_unit_KS4)(sample_idx[uid], data, samples_before, samples_after, spike_width, n_channels, sample_amount)for uid in range(nUnits))\n",
    "            avg_waveforms = np.asarray(avg_waveforms)           \n",
    "        else:\n",
    "            avg_waveforms = Parallel(n_jobs = -1, verbose = 10, mmap_mode='r', max_nbytes=None )(delayed(erd.extract_a_unit)(sample_idx[uid], data, half_width, spike_width, n_channels, sample_amount)for uid in range(nUnits))\n",
    "            avg_waveforms = np.asarray(avg_waveforms)\n",
    "\n",
    "        #Save in file named 'RawWaveforms' in the KS Directory\n",
    "        erd.save_avg_waveforms(avg_waveforms, KS_dirs[sid], GoodUnits = good_units[sid], extract_good_units_only = extract_good_units_only)\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: delete the decompressed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "#DELETE the decompressed data Directory/Folder ( i.e multiple sessions)\n",
    "shutil.rmtree(decomp_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
