{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This demo notebook can be used to (optionally) decompress ephys data and create two average waveforms per session needed for Unit Match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import UnitMatchPy.extract_raw_data as erd\n",
    "import numpy as np \n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give paramaters and paths needed for extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Up Parameters\n",
    "sample_amount = 1000 # for both CV, at least 500 per CV\n",
    "spike_width = 82 # assuming 30khz sampling, 82 and 61 are common choices (KS1-3, KS4), covers the AP and space around needed for processing\n",
    "half_width = np.floor(spike_width/2).astype(int)\n",
    "max_width = np.floor(spike_width/2).astype(int) #Size of area at start and end of recording to ignore to get only full spikes\n",
    "n_channels = 384 #neuropixels default, the number of channels EXCLUDING sync channels\n",
    "extract_good_units_only = False # bool, set to true if you want to only extract units marked as good \n",
    "\n",
    "KS4_data = False #bool, set to true if using Kilosort, as KS4 spike times refer to start of waveform not peak\n",
    "if KS4_data:\n",
    "    samples_before = 20\n",
    "    samples_after = spike_width - samples_before\n",
    "    max_width = samples_after #Number of samples on either side of the \n",
    "\n",
    "#List of paths to a KS directory, can pass paths \n",
    "KS_dirs = [r'path/to/KiloSort/Dir/Session1', r'path/to/KiloSort/Dir/Session2']\n",
    "n_sessions = len(KS_dirs) #How many session are being extracted\n",
    "spike_ids, spike_times, good_units, all_unit_ids = erd.extract_KS_data(KS_dirs, extract_good_units_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need decompressed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give metadata + Raw data paths\n",
    "\n",
    "data_paths = [r'path/to/Decompressed/data1.dat', r'path/to/Decompressed/data2.dat']\n",
    "meta_paths = [r'path/to/data/structure.oebin', r'path/to/data/structure.oebin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the units \n",
    "\n",
    "if extract_good_units_only:\n",
    "    for sid in range(n_sessions):\n",
    "        #load metadata\n",
    "        with open(meta_paths[sid], 'r') as file:\n",
    "            meta = json.load(file)\n",
    "        n_bytes = os.path.getsize(data_paths[sid])\n",
    "        n_channels_tot = int(meta['continuous'][0]['num_channels'])\n",
    "        n_samples = int(n_bytes / (2*n_channels_tot))\n",
    "\n",
    "        #create memmap to raw data, for that session\n",
    "        data = np.memmap(data_paths[sid], dtype = 'int16', shape =(n_samples, n_channels_tot))\n",
    "\n",
    "        # Remove spike which won't have a full waveform recorded\n",
    "        spike_ids_tmp = np.delete(spike_ids[sid], np.logical_or( (spike_times[sid] < max_width), ( spike_times[sid] > (data.shape[0] - max_width))))\n",
    "        spike_times_tmp = np.delete(spike_times[sid], np.logical_or( (spike_times[sid] < max_width), ( spike_times[sid] > (data.shape[0] - max_width))))\n",
    "\n",
    "\n",
    "        #might be slow extracting sample for good units only?\n",
    "        sample_idx = erd.get_sample_idx(spike_times_tmp, spike_ids_tmp, sample_amount, units = good_units[sid])\n",
    "\n",
    "        if KS4_data:\n",
    "            avg_waveforms = Parallel(n_jobs = -1, verbose = 10, mmap_mode='r', max_nbytes=None )(delayed(erd.extract_a_unit_KS4)(sample_idx[uid], data, samples_before, samples_after, spike_width, n_channels, sample_amount)for uid in range(good_units[sid].shape[0]))\n",
    "            avg_waveforms = np.asarray(avg_waveforms)           \n",
    "        else:\n",
    "            avg_waveforms = Parallel(n_jobs = -1, verbose = 10, mmap_mode='r', max_nbytes=None )(delayed(erd.extract_a_unit)(sample_idx[uid], data, half_width, spike_width, n_channels, sample_amount)for uid in range(good_units[sid].shape[0]))\n",
    "            avg_waveforms = np.asarray(avg_waveforms)\n",
    "\n",
    "        #Save in file named 'RawWaveforms' in the KS Directory\n",
    "        erd.save_avg_waveforms(avg_waveforms, KS_dirs[sid], all_unit_ids[sid], GoodUnits = good_units[sid], extract_good_units_only = extract_good_units_only)\n",
    "\n",
    "else:\n",
    "    for sid in range(n_sessions):\n",
    "        #Extracting ALL the Units\n",
    "        n_units = len(np.unique(spike_ids[sid]))\n",
    "        #load metadata\n",
    "        with open(meta_paths[sid], 'r') as file:\n",
    "            meta = json.load(file)\n",
    "        n_bytes = os.path.getsize(data_paths[sid])\n",
    "        n_channels_tot = int(meta['continuous'][0]['num_channels'])\n",
    "        n_samples = int(n_bytes / (2*n_channels_tot))\n",
    "\n",
    "        #create memmap to raw data, for that session\n",
    "        data = np.memmap(data_paths[sid], dtype = 'int16', shape =(n_samples, n_channels_tot))\n",
    "\n",
    "        # Remove spikes which won't have a full waveform recorded\n",
    "        spike_ids_tmp = np.delete(spike_ids[sid], np.logical_or( (spike_times[sid] < max_width), ( spike_times[sid] > (data.shape[0] - max_width))))\n",
    "        spike_times_tmp = np.delete(spike_times[sid], np.logical_or( (spike_times[sid] < max_width), ( spike_times[sid] > (data.shape[0] - max_width))))\n",
    "\n",
    "\n",
    "        sample_idx = erd.get_sample_idx(spike_times_tmp, spike_ids_tmp, sample_amount, units= np.unique(spike_ids[sid]))\n",
    "        \n",
    "        if KS4_data:\n",
    "            avg_waveforms = Parallel(n_jobs = -1, verbose = 10, mmap_mode='r', max_nbytes=None )(delayed(erd.extract_a_unit_KS4)(sample_idx[uid], data, samples_before, samples_after, spike_width, n_channels, sample_amount)for uid in range(n_units))\n",
    "            avg_waveforms = np.asarray(avg_waveforms)           \n",
    "        else:\n",
    "            avg_waveforms = Parallel(n_jobs = -1, verbose = 10, mmap_mode='r', max_nbytes=None )(delayed(erd.extract_a_unit)(sample_idx[uid], data, half_width, spike_width, n_channels, sample_amount)for uid in range(n_units))\n",
    "            avg_waveforms = np.asarray(avg_waveforms)\n",
    "\n",
    "        #Save in file named 'RawWaveforms' in the KS Directory\n",
    "        erd.save_avg_waveforms(avg_waveforms, KS_dirs[sid], all_unit_ids[sid], GoodUnits = good_units[sid], extract_good_units_only = extract_good_units_only)\n",
    "del data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
