{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This demo notebook can be used to (optionally) decompress ephys data and create two average waveforms per session needed for Unit Match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import UnitMatchPy.Extract_raw_data as erd\n",
    "import numpy as np \n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional, decompress compressed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVE a list of dirs, for where the raw compressed data is e.g .cbim, .ch and .meta files\n",
    "RawDataDirPaths = [r'Path\\to\\rawdata\\Session1', r'Path\\to\\rawdata\\Session2']\n",
    "\n",
    "#Path to a directory where you want the decompresed data to be saved\n",
    "#this is a large file, using a fast ssd is advised for quicker run times\n",
    "\n",
    "#GIVE a path to a directory where the Decompressed data will be saved\n",
    "#this will make folder called session n for each session\n",
    "DecompDataSaveDir = r'path/to/decompressed/data/save/dir'\n",
    "\n",
    "cbinPaths, chPaths, metaPaths = erd.get_raw_data_paths(RawDataDirPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decompress Data\n",
    "from mtscomp import Reader\n",
    "\n",
    "DecompDir = os.path.join(DecompDataSaveDir, 'DecompData')\n",
    "os.mkdir(DecompDir) # Create a folder in the directory called 'DecompData'\n",
    "\n",
    "DataPaths = []\n",
    "for i in range(len(RawDataDirPaths)):\n",
    "    tmpPath = os.path.join(DecompDir, f'Session{i+1}')  #+1 so starts at 1\n",
    "    os.mkdir(tmpPath) # make a folder for each session called 'SessionX' \n",
    "    tmpPath = os.path.join(tmpPath, 'RawData.bin')\n",
    "    DataPaths.append(tmpPath)\n",
    "\n",
    "    # create .bin with the decompressed data\n",
    "\n",
    "    #r = Reader() # do the mtscomp verification\n",
    "    r = Reader(check_after_decompress = False) #Skip the verification check to save time\n",
    "    r.open(cbinPaths[i], chPaths[i])\n",
    "    r.tofile(tmpPath)\n",
    "    r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give paramaters and paths needed for extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Up Parameters\n",
    "SampleAmount = 1000 # for both CV, at least 500 per CV\n",
    "SpikeWidth = 82 # assuming 30khz sampling, 82 and 61 are common choices, covers the AP and space around needed for processing\n",
    "HalfWidth = np.floor(SpikeWidth/2).astype(int)\n",
    "nChannels = 384 #neuropixels default\n",
    "ExtractGoodUnitsOnly = False # bool, set to true if you want to only extract units marked as good \n",
    "\n",
    "KS4data = False #bool, set to true if using Kilosort, as KS4 spiketimes refer to start of waveform not peak\n",
    "if KS4data:\n",
    "    SamplesBefore = 20\n",
    "    SamplesAfter = SpikeWidth - SamplesBefore\n",
    "\n",
    "#List of paths to a KS directory, can pass paths \n",
    "KSdirs = [r'path/to/KiloSort/Dir/Session1', r'path/to/KiloSort/Dir/Session2']\n",
    "nSessions = len(KSdirs) #How many session are being extracted\n",
    "SpikeIds, SpikeTimes, GoodUnits = erd.extract_KSdata(KSdirs, ExtractGoodUnitsOnly = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have not decompressed data above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give metadata + Raw data paths\n",
    "#if you are NOT decompressing data here, provide a list of paths to the decompressed data and the metadata\n",
    "\n",
    "\n",
    "#DataPaths = [r'path/to/Decompressed/data1.bin', r'path/to/Decompressed/data2.bin']\n",
    "#metaPaths = [r''path/to/data.meta', r'path/to/data.meta']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GoodUnits = [None for s in range(nSessions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the units \n",
    "\n",
    "if ExtractGoodUnitsOnly:\n",
    "    for sid in range(nSessions):\n",
    "        #load metadata\n",
    "        MetaData = erd.Read_Meta(Path(metaPaths[sid]))\n",
    "        nElements = int(MetaData['fileSizeBytes']) / 2\n",
    "        nChannelsTot = int(MetaData['nSavedChans'])\n",
    "\n",
    "        #create memmap to raw data, for that session\n",
    "        Data = np.memmap(DataPaths[sid], dtype = 'int16', shape =(int(nElements / nChannelsTot), nChannelsTot))\n",
    "\n",
    "        # Remove spike which won't have a full wavefunction recorded\n",
    "        SpikeIdsTmp = np.delete(SpikeIds[sid], np.logical_or( (SpikeTimes[sid] < HalfWidth), ( SpikeTimes[sid] > (Data.shape[0] - HalfWidth))))\n",
    "        SpikeTimesTmp = np.delete(SpikeTimes[sid], np.logical_or( (SpikeTimes[sid] < HalfWidth), ( SpikeTimes[sid] > (Data.shape[0] - HalfWidth))))\n",
    "\n",
    "\n",
    "        #might be slow extracting smaple for good units only?\n",
    "        SampleIdx = erd.get_sample_idx(SpikeTimesTmp, SpikeIdsTmp, SampleAmount, units = GoodUnits[sid])\n",
    "\n",
    "        if KS4data:\n",
    "            AvgWaveforms = Parallel(n_jobs = -1, verbose = 10, mmap_mode='r', max_nbytes=None )(delayed(erd.Extract_A_UnitKS4)(SampleIdx[uid], Data, SamplesBefore, SamplesAfter, SpikeWidth, nChannels, SampleAmount)for uid in range(GoodUnits[sid].shape[0]))\n",
    "            AvgWaveforms = np.asarray(AvgWaveforms)           \n",
    "        else:\n",
    "            AvgWaveforms = Parallel(n_jobs = -1, verbose = 10, mmap_mode='r', max_nbytes=None )(delayed(erd.Extract_A_Unit)(SampleIdx[uid], Data, HalfWidth, SpikeWidth, nChannels, SampleAmount)for uid in range(GoodUnits[sid].shape[0]))\n",
    "            AvgWaveforms = np.asarray(AvgWaveforms)\n",
    "\n",
    "        #Save in file named 'RawWaveforms' in the KS Directory\n",
    "        erd.Save_AvgWaveforms(AvgWaveforms, KSdirs[sid], GoodUnits = GoodUnits[sid], ExtractGoodUnitsOnly = ExtractGoodUnitsOnly)\n",
    "\n",
    "else:\n",
    "    for sid in range(nSessions):\n",
    "        #Extracting ALL the Units\n",
    "        nUnits = len(np.unique(SpikeIds[sid]))\n",
    "        #load metadata\n",
    "        MetaData = erd.Read_Meta(Path(metaPaths[sid]))\n",
    "        nElements = int(MetaData['fileSizeBytes']) / 2\n",
    "        nChannelsTot = int(MetaData['nSavedChans'])\n",
    "\n",
    "        #create memmap to raw data, for that session\n",
    "        Data = np.memmap(DataPaths[sid], dtype = 'int16', shape =(int(nElements / nChannelsTot), nChannelsTot))\n",
    "\n",
    "        # Remove spike which won't have a full wavefunction recorded\n",
    "        SpikeIdsTmp = np.delete(SpikeIds[sid], np.logical_or( (SpikeTimes[sid] < HalfWidth), ( SpikeTimes[sid] > (Data.shape[0] - HalfWidth))))\n",
    "        SpikeTimesTmp = np.delete(SpikeTimes[sid], np.logical_or( (SpikeTimes[sid] < HalfWidth), ( SpikeTimes[sid] > (Data.shape[0] - HalfWidth))))\n",
    "\n",
    "\n",
    "        SampleIdx = erd.get_sample_idx(SpikeTimesTmp, SpikeIdsTmp, SampleAmount, units= np.unique(SpikeIds[sid]))\n",
    "        \n",
    "        if KS4data:\n",
    "            AvgWaveforms = Parallel(n_jobs = -1, verbose = 10, mmap_mode='r', max_nbytes=None )(delayed(erd.Extract_A_UnitKS4)(SampleIdx[uid], Data, SamplesBefore, SamplesAfter, SpikeWidth, nChannels, SampleAmount)for uid in range(nUnits))\n",
    "            AvgWaveforms = np.asarray(AvgWaveforms)           \n",
    "        else:\n",
    "            AvgWaveforms = Parallel(n_jobs = -1, verbose = 10, mmap_mode='r', max_nbytes=None )(delayed(erd.Extract_A_Unit)(SampleIdx[uid], Data, HalfWidth, SpikeWidth, nChannels, SampleAmount)for uid in range(nUnits))\n",
    "            AvgWaveforms = np.asarray(AvgWaveforms)\n",
    "\n",
    "        #Save in file named 'RawWaveforms' in the KS Directory\n",
    "        erd.Save_AvgWaveforms(AvgWaveforms, KSdirs[sid], GoodUnits = GoodUnits[sid], ExtractGoodUnitsOnly = ExtractGoodUnitsOnly)\n",
    "del Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: delete the decompressed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "#DELETE the decompressed data Directory/Folder ( i.e multiple sessiosn)\n",
    "shutil.rmtree(DecompDir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
