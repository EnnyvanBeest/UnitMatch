{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook/demo uses spike interface, [here is how to install Spike Interface](https://spikeinterface.readthedocs.io/en/latest/installation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import spikeinterface as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "import UMPy.Extract_raw_data as erd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & get good units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spike Interface can load in many different types of Ephys data look [here](https://spikeinterface.readthedocs.io/en/latest/modules/extractors.html) for documentation on fucntion to read in different data formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make list of recordings/sortings to iterate over\n",
    "Recordings = [se.read_spikeglx(r'path/to/SpikeGLX/data', stream_name=\"imec0.ap\")]\n",
    "\n",
    "Sortings = [se.read_kilosort(r'Path/To/KiloSort/Directory')]\n",
    "\n",
    "#Will only make average wavefroms for good units\n",
    "ExtractGoodUnitsOnly = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting good units only\n",
    "Sortings[0].get_property_keys() #lists keys for attached propties if 'quality' is not suitbale\n",
    "\n",
    "#Good units which will be used in Unit Match\n",
    "GoodUnits = []\n",
    "UnitsUsed = []\n",
    "for i, sorting in enumerate(Sortings):\n",
    "    UnitIdsTmp = sorting.get_property('original_cluster_id')\n",
    "    IsGoodTmp = sorting.get_property('quality')\n",
    "    GoodUnits.append(np.stack((UnitIdsTmp,IsGoodTmp), axis = 1))\n",
    "\n",
    "    UnitsUsed.append(UnitIdsTmp)\n",
    "    if ExtractGoodUnitsOnly is True:\n",
    "        keep = np.argwhere(IsGoodTmp == 'good').squeeze()\n",
    "        Sortings[i] = sorting.select_units(keep)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process average waveforms / templates\n",
    "\n",
    "Beaware the spike interface method is different to the native unitmatch method in ExtractRawDemo.ipynb or in the MatLab version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocces the raw data\n",
    "for recording in Recordings:\n",
    "    recording = spre.phase_shift(recording) #correct for time delay between recording channels\n",
    "    recording = spre.highpass_filter(recording) #highpass\n",
    "\n",
    "    # for motion correction, this can be very slow\n",
    "    #Uncommented code below to do in session motion correction\n",
    "    #recording = spre.correct_motion(recording, preset=\"nonrigid_fast_and_accurate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split each recording/sorting into 2 halves                    \n",
    "for i, sorting in enumerate(Sortings):\n",
    "    SplitIdx = Recordings[i].get_num_samples() // 2\n",
    "\n",
    "    SplitSorting = []\n",
    "    SplitSorting.append(sorting.frame_slice(start_frame=0, end_frame=SplitIdx))\n",
    "    SplitSorting.append(sorting.frame_slice(start_frame=SplitIdx, end_frame=Recordings[i].get_num_samples()))\n",
    "\n",
    "    Sortings[i] = SplitSorting \n",
    "\n",
    "for i, recording in enumerate(Recordings):\n",
    "    SplitIdx = recording.get_num_samples() // 2\n",
    "\n",
    "    SplitRecording = []\n",
    "    SplitRecording.append(recording.frame_slice(start_frame=0, end_frame=SplitIdx))\n",
    "    SplitRecording.append(recording.frame_slice(start_frame=SplitIdx, end_frame=recording.get_num_samples()))\n",
    "\n",
    "    Recordings[i] = SplitRecording\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sorting analyzer for each pair\n",
    "Analysers = []\n",
    "for i in range(len(Recordings)):\n",
    "    SplitAnalysers = []\n",
    "\n",
    "    SplitAnalysers.append(si.create_sorting_analyzer(Sortings[i][0], Recordings[i][0], sparse=False))\n",
    "    SplitAnalysers.append(si.create_sorting_analyzer(Sortings[i][1], Recordings[i][1], sparse=False))\n",
    "    Analysers.append(SplitAnalysers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the fast tempalte extension for each sorting analyser\n",
    "AllWaveforms = []\n",
    "for i in range(len(Analysers)):\n",
    "    for half in range(2):\n",
    "        Analysers[i][half].compute(\n",
    "            \"random_spikes\",\n",
    "            method=\"uniform\",\n",
    "            max_spikes_per_unit=500)\n",
    "        \n",
    "        #Analysers[i][half].compute('fast_templates', n_jobs = 0.8,  return_scaled=True)\n",
    "        Analysers[i][half].compute('fast_templates', n_jobs = 0.8)\n",
    "    \n",
    "    TemplatesFirst = Analysers[i][0].get_extension('fast_templates')\n",
    "    TemplatesSecond = Analysers[i][1].get_extension('fast_templates')\n",
    "    t1 = TemplatesFirst.get_data()\n",
    "    t2 = TemplatesSecond.get_data()\n",
    "    AllWaveforms.append(np.stack((t1,t2), axis = -1))\n",
    "\n",
    "#Make a channel_postions array\n",
    "AllPositions = []\n",
    "for i in range(len(Analysers)):\n",
    "    #postions for first half and second half are the same\n",
    "    AllPositions.append(Analysers[i][0].get_channel_locations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save extracted data in a unit match friendly folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "UMInputDir = os.path.join(os.getcwd(), 'UMInputData')\n",
    "\n",
    "os.mkdir(UMInputDir)\n",
    "\n",
    "AllSessionPaths = []\n",
    "for i in range(len(Recordings)):\n",
    "    SessionXpath = os.path.join(UMInputDir, f'Session{i+1}') #lets start at 1\n",
    "    os.mkdir(SessionXpath)\n",
    "\n",
    "    #save the GoodUnits as a .rsv first column is unit ID,second is 'good' or 'mua'\n",
    "    GoodUnitsPath = os.path.join(SessionXpath, 'cluster_group.tsv')\n",
    "    ChannelPositionsPath = os.path.join(SessionXpath, 'channel_positions.npy')\n",
    "    SaveGoodUnits = np.vstack((np.array(('cluster_id', 'group')), GoodUnits[i])) #Title of colum one is '0000' Not 'cluster_id')\n",
    "    SaveGoodUnits[0,0] = 0 # need to be int to use np.savetxt \n",
    "    np.savetxt(GoodUnitsPath, SaveGoodUnits, fmt =['%d','%s'], delimiter='\\t')\n",
    "    if ExtractGoodUnitsOnly:\n",
    "        Units = np.argwhere(GoodUnits[0][:,1] == 'good')\n",
    "        erd.Save_AvgWaveforms(AllWaveforms[i], SessionXpath, Units, ExtractGoodUnitsOnly = ExtractGoodUnitsOnly)\n",
    "    else:\n",
    "        erd.Save_AvgWaveforms(AllWaveforms[i], SessionXpath, GoodUnits[i], ExtractGoodUnitsOnly = ExtractGoodUnitsOnly)\n",
    "    np.save(ChannelPositionsPath, AllPositions[i])\n",
    "\n",
    "    AllSessionPaths.append(SessionXpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run UnitMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "import Bayes_fun as bf\n",
    "import utils as util\n",
    "import Overlord\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import GUI as gui\n",
    "import Save_utils as su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get default parameters, can add your own before or after!\n",
    "\n",
    "# default of Spikeinterface as by default spike interface extracts waveforms in a different manner.\n",
    "param = {'SpikeWidth': 90, 'waveidx': np.arange(20,50), 'PeakLoc': 35}\n",
    "param = util.get_default_param(param)\n",
    "\n",
    "WavePaths, UnitLabelPaths, ChannelPos = util.paths_fromKS(AllSessionPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_center_waveform(waveform):\n",
    "    \"\"\"\n",
    "    Centers waveform about zero, by subtracting the mean of the first 15 time points.\n",
    "    This function is useful for Spike Interface where the waveforms are not centered about 0.\n",
    "\n",
    "    Arguments:\n",
    "        waveform - ndarray (nUnits, Time Points, Channels, CV)\n",
    "\n",
    "    Returns:\n",
    "        Zero centered waveform\n",
    "    \"\"\"\n",
    "    waveform = waveform -  np.broadcast_to(waveform[:,:15,:,:].mean(axis=1)[:, np.newaxis,:,:], waveform.shape)\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data and select the good units and exact metadata\n",
    "waveform, SessionID, SessionSwitch, WithinSession, GoodUnits, param = util.load_good_waveforms(WavePaths, UnitLabelPaths, param) # 1-step version of above\n",
    "\n",
    "#Recenter the waveform at 0, as is not part of spike interface tempaltes \n",
    "waveform = zero_center_waveform(waveform)\n",
    "\n",
    "# create clusInfo, contains all unit id/session related info\n",
    "ClusInfo = {'GoodUnits' : GoodUnits, 'SessionSwitch' : SessionSwitch, 'SessionID' : SessionID, \n",
    "            'OriginalID' : np.concatenate(GoodUnits) }\n",
    "\n",
    "#Extract parameters from waveform\n",
    "ExtractedWaveProperties = Overlord.extract_parameters(waveform, ChannelPos, ClusInfo, param)\n",
    "\n",
    "#Extract metric scores\n",
    "TotalScore, CandidatePairs, Scores2Include, Predictors  = Overlord.extract_metric_scores(ExtractedWaveProperties, SessionSwitch, WithinSession, param, niter  = 2)\n",
    "\n",
    "#Probability analysis\n",
    "priorMatch = 1 - (param['nExpectedMatches'] / param['nUnits']**2 ) # fredom of choose in prior prob?\n",
    "Priors = np.array((priorMatch, 1-priorMatch))\n",
    "\n",
    "labels = CandidatePairs.astype(int)\n",
    "Cond = np.unique(labels)\n",
    "ScoreVector = param['ScoreVector']\n",
    "ParameterKernels = np.full((len(ScoreVector), len(Scores2Include), len(Cond)), np.nan)\n",
    "\n",
    "ParameterKernels = bf.get_ParameterKernels(Scores2Include, labels, Cond, param, addone = 1)\n",
    "\n",
    "Probability = bf.apply_naive_bayes(ParameterKernels, Priors, Predictors, param, Cond)\n",
    "\n",
    "Output = Probability[:,1].reshape(param['nUnits'],param['nUnits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.evaluate_output(Output, param, WithinSession, SessionSwitch, MatchThreshold = 0.75)\n",
    "\n",
    "MatchThreshold = 0.75\n",
    "OutputThreshold = np.zeros_like(Output)\n",
    "OutputThreshold[Output > MatchThreshold] = 1\n",
    "\n",
    "plt.imshow(OutputThreshold, cmap = 'Greys')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Amplitude = ExtractedWaveProperties['Amplitude']\n",
    "SpatialDecay = ExtractedWaveProperties['SpatialDecay']\n",
    "AvgCentroid = ExtractedWaveProperties['AvgCentroid']\n",
    "AvgWaveform = ExtractedWaveProperties['AvgWaveform']\n",
    "AvgWaveformPerTP = ExtractedWaveProperties['AvgWaveformPerTP']\n",
    "WaveIdx = ExtractedWaveProperties['WaveIdx']\n",
    "MaxSite = ExtractedWaveProperties['MaxSite']\n",
    "MaxSiteMean = ExtractedWaveProperties['MaxSiteMean']\n",
    "gui.process_info_for_GUI(Output, MatchThreshold, Scores2Include, TotalScore, Amplitude, SpatialDecay,\n",
    "                         AvgCentroid, AvgWaveform, AvgWaveformPerTP, WaveIdx, MaxSite, MaxSiteMean, \n",
    "                         waveform, WithinSession, ChannelPos, ClusInfo, param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IsMatch, NotMatch, MatchesGUI = gui.run_GUI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matches = np.argwhere(MatchThreshold == 1)\n",
    "\n",
    "SaveDir = r'Path/to/save/directory'\n",
    "su.save_to_output(SaveDir, Scores2Include, Matches, Output, AvgCentroid, AvgWaveform, AvgWaveformPerTP, MaxSite,\n",
    "                   TotalScore, OutputThreshold, ClusInfo, param, MatchesCurated = None, SaveMatchTable = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
