{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "import Extract_raw_data as erd\n",
    "import numpy as np \n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function needs decompressed data, the cell below can decompress and save raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decompress n sessions at a time, n can be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVE a list of dirs, for where the raw compressed data is e.g .cbim, .ch and .meta files\n",
    "RawDataDirPaths = [r'Path\\to\\rawdata\\Session1', r'Path\\to\\rawdata\\Session2']\n",
    "\n",
    "cbinPaths, chPaths, metaPaths = erd.get_raw_data_paths(RawDataDirPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decompressing:   0%|          | 0/164 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decompressing: 100%|██████████| 164/164 [02:31<00:00,  1.08it/s]\n",
      "INFO:mtscomp:Wrote c:\\Users\\Experiment\\Data\\DecompData\\Session1\\RawData.bin (42.2 GB).\n"
     ]
    }
   ],
   "source": [
    "#Decompress Data\n",
    "from mtscomp import Reader\n",
    "\n",
    "#Path to a directory where you want the decompresed data to be saved\n",
    "#this is a large file, using a ssd is advised for quicker run times\n",
    "\n",
    "#GIVE a path to a directory where the Decompressed data will be saved\n",
    "#will make folder called session n for each session\n",
    "#DecompDataSaveDir = r'path/to/decompressed/data/save/dir'\n",
    "DecompDataSaveDir = r'c:\\Users\\Experiment\\Data'\n",
    "DecompDir = os.path.join(DecompDataSaveDir, 'DecompData')\n",
    "os.mkdir(DecompDir) # Create a folder in the directory called 'DecompData'\n",
    "\n",
    "\n",
    "DataPaths = []\n",
    "for i in range(len(RawDataDirPaths)):\n",
    "    tmpPath = os.path.join(DecompDir, f'Session{i+1}')  #+1 so starts at 1\n",
    "    os.mkdir(tmpPath) # make a folder for each session called 'SessionX' \n",
    "    tmpPath = os.path.join(tmpPath, 'RawData.bin')\n",
    "    DataPaths.append(tmpPath)\n",
    "\n",
    "    # create .bin with the decompressed data\n",
    "\n",
    "    #r = Reader() # do the mtscomp verification\n",
    "    r = Reader(check_after_decompress = False) #Skip the verification check to save time\n",
    "    r.open(cbinPaths[i], chPaths[i])\n",
    "    r.tofile(tmpPath)\n",
    "    r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract average waveforms from decompressed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Up Parameters\n",
    "SampleAmount = 1000 # atleast 500 per CV\n",
    "SpikeWidth = 82 # assuming 30khz sampling, UM standard, covers the AP and space around needed for processing\n",
    "HalfWidth = np.floor(SpikeWidth/2).astype(int)\n",
    "nChannels = 384\n",
    "ExtractGoodUnitsOnly = True \n",
    "\n",
    "#List of paths to a KS dir\n",
    "KSdirs = [r'path/to/KiloSort/Dir/Session1', r'path/to/KiloSort/Dir/Session2']\n",
    "nSessions = len(KSdirs) #How many session are being extracted\n",
    "SpikeIds, SpikeTimes, GoodUnits = erd.extract_KSdata(KSdirs, ExtractGoodUnitsOnly = True)\n",
    "\n",
    "KSdirs[0] = r'c:\\Users\\Experiment\\Data\\DecompData'\n",
    "#give metadata + Raw data paths\n",
    "#if you are NOT decompressing data here, provide a list of paths to the decompressed data and the metadata\n",
    "\n",
    "#DataPaths = [r'c:\\Users\\Experiment\\Data\\RawData.bin']\n",
    "#metaPaths = [r'']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:   28.4s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed:   34.6s\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed:   37.6s\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed:   41.5s\n",
      "[Parallel(n_jobs=-1)]: Done 278 out of 278 | elapsed:   46.4s finished\n"
     ]
    }
   ],
   "source": [
    "#Extract the units \n",
    "\n",
    "if ExtractGoodUnitsOnly:\n",
    "    for sid in range(nSessions):\n",
    "        #load metadata\n",
    "        MetaData = erd.Read_Meta(Path(metaPaths[sid]))\n",
    "        nElements = int(MetaData['fileSizeBytes']) / 2\n",
    "        nChannelsTot = int(MetaData['nSavedChans'])\n",
    "\n",
    "        #create memmap to raw data, for that session\n",
    "        Data = np.memmap(DataPaths[sid], dtype = 'int16', shape =(int(nElements / nChannelsTot), nChannelsTot))\n",
    "\n",
    "        # Remove spike which won't have a full wavefunction recorded\n",
    "        SpikeIdsTmp = np.delete(SpikeIds[sid], np.logical_or( (SpikeTimes[sid] < HalfWidth), ( SpikeTimes[sid] > (Data.shape[0] - HalfWidth))))\n",
    "        SpikeTimesTmp = np.delete(SpikeTimes[sid], np.logical_or( (SpikeTimes[sid] < HalfWidth), ( SpikeTimes[sid] > (Data.shape[0] - HalfWidth))))\n",
    "\n",
    "\n",
    "        #might be slow extracting smaple for good units only?\n",
    "        SampleIdx = erd.get_sample_idx(SpikeTimesTmp, SpikeIdsTmp, SampleAmount, units = GoodUnits[sid])\n",
    "\n",
    "        AvgWaveforms = Parallel(n_jobs = -1, verbose = 10, mmap_mode='r', max_nbytes=None )(delayed(erd.Extract_A_Unit)(SampleIdx[uid], Data, HalfWidth, SpikeWidth, nChannels, SampleAmount)for uid in range(GoodUnits[sid].shape[0]))\n",
    "        AvgWaveforms = np.asarray(AvgWaveforms)\n",
    "\n",
    "        #Save in file named 'RawWaveforms' in the KS Directory\n",
    "        erd.Save_AvgWaveforms(AvgWaveforms, KSdirs[sid], GoodUnits = GoodUnits[sid], ExtractGoodUnitsOnly = ExtractGoodUnitsOnly)\n",
    "\n",
    "else:\n",
    "    for sid in range(nSessions):\n",
    "        #Extracting ALL the Units\n",
    "        nUnits = len(np.unique(SpikeIds[sid]))\n",
    "        #load metadata\n",
    "        MetaData = erd.Read_Meta(Path(metaPaths[sid]))\n",
    "        nElements = int(MetaData['fileSizeBytes']) / 2\n",
    "        nChannelsTot = int(MetaData['nSavedChans'])\n",
    "\n",
    "        #create memmap to raw data, for that session\n",
    "        Data = np.memmap(DataPaths[sid], dtype = 'int16', shape =(int(nElements / nChannelsTot), nChannelsTot))\n",
    "\n",
    "        # Remove spike which won't have a full wavefunction recorded\n",
    "        SpikeIdsTmp = np.delete(SpikeIds[sid], np.logical_or( (SpikeTimes[sid] < HalfWidth), ( SpikeTimes[sid] > (Data.shape[0] - HalfWidth))))\n",
    "        SpikeTimesTmp = np.delete(SpikeTimes[sid], np.logical_or( (SpikeTimes[sid] < HalfWidth), ( SpikeTimes[sid] > (Data.shape[0] - HalfWidth))))\n",
    "\n",
    "\n",
    "        SampleIdx = erd.get_sample_idx(SpikeTimesTmp, SpikeIdsTmp, SampleAmount, units= np.unique(SpikeIds[sid]))\n",
    "        AvgWaveforms = Parallel(n_jobs = -1, verbose = 10, mmap_mode='r', max_nbytes=None )(delayed(erd.Extract_A_Unit)(SampleIdx[uid], Data, HalfWidth, SpikeWidth, nChannels, SampleAmount)for uid in range(nUnits))\n",
    "        AvgWaveforms = np.asarray(AvgWaveforms)\n",
    "\n",
    "        #Save in file named 'RawWaveforms' in the KS Directory\n",
    "        erd.Save_AvgWaveforms(AvgWaveforms, KSdirs[sid])\n",
    "\n",
    "del Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below will delete the decomressed data, can delete in file explorer as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "#DELETE the decompressed data Directory/Folder ( i.e multiple sessiosn)\n",
    "shutil.rmtree(DecompDir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
